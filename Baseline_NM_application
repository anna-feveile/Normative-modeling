"""
            ### DATA PREPARATION ###
#Import os and change directory to where braincharts folder should be located
import os
os.chdir("/path/to/braincharts_folder")

# Install pcntoolkit and clone github repository
! pip install pcntoolkit==0.28
! git clone https://github.com/predictive-clinical-neuroscience/braincharts.git


# Change directory to scripts folder and import necessary libraries
os.chdir("/braincharts/scripts/")

import numpy as np
import pandas as pd
import pickle
from matplotlib import pyplot as plt
import seaborn as sns

from pcntoolkit.normative import estimate, predict, evaluate
from pcntoolkit.util.utils import compute_MSLL, create_design_matrix
from nm_utils import remove_bad_subjects, load_2d

# Change directory to model folder
os.chdir("/braincharts/models/")
  # life_span_57K_82sites model was unzipped manually

# Choose pretrained model (trained on approx. N=57000 subjects with 82 sites)
model_name = "lifespan_57K_82sites"
site_names = "site_ids_ct_82sites.txt"

# Set directories
    # Analysis directory
root_dir = "/braincharts"

    # Data file directory
data_dir = "/braincharts/docs"

    # Model directory
out_dir = os.path.join(root_dir, "models", model_name)

# Load site ids from the model
with open(os.path.join(root_dir,"docs", site_names)) as f:
    site_ids_tr = f.read().splitlines()


# Load all baseline data
all_data = os.path.join(data_dir, "Baseline_data_file.csv")
df_all = pd.read_csv(all_data)

# Insert group column (0 = HC, 1 = FEP) and divide dataframe based on group
df_all.insert(3, "Group", True)
df_all['Group']=0

df_all["Group"][df_all["Id"].str.contains("FEP_PECANS_identifier")==True]=1
df_all["Group"][df_all["Id"].str.contains("FEP_PECANSII_identifier")==True]=1

# Change site (PECANS = GLO1, PECANSII = GLO2) and sitenum (PECANS = 0, PECANSII = 1)
df_all["Site"][df_all["Id"].str.contains("PECANS_identifier")==True]="GLO1"
df_all["Site"][df_all["Id"].str.contains("PECANSII_identifier")==True]="GLO2"
df_all["Sitenum"][df_all["Id"].str.contains("PECANS_identifier")==True]=0
df_all["Sitenum"][df_all["Id"].str.contains("PECANSII_identifier")==True]=1

# Remove subjects that should be excluded
    #Excluded due to MR finding
df_all = df_all[df_all["Id"].str.contains("subject1|subject2|subject3|subject4|subject5|subject6|subject7")==False]

    #Excluded due to non-SCZ diagnosis
df_all = df_all[df_all["Id"].str.contains("subject1|subject2")==False]

    #Excluded due to poor MR image or segmentation quality
df_all = df_all[df_all["Id"].str.contains("subject1|subject2|subject3")==False]

# Choose adaptation data (from HC subjects)
from sklearn.model_selection import train_test_split

df_ad = df_all.loc[df_all["Group"]==0]
ad_data, HC_data = train_test_split(df_ad, train_size=0.4, shuffle=True, random_state=25, stratify=df_ad[["Sex", "Sitenum"]])
df_ad = ad_data

df_te = pd.merge(df_all, df_ad, indicator=True, how='outer').query('_merge=="left_only"').drop('_merge', axis=1)

# Save test and adaptation data
os.chdir(data_dir)
df_ad.to_csv("Adaptation_data.csv", index=False)
df_te.to_csv("Test_data.csv", index=False)
    
# Load test and adaptation data
test_data = os.path.join(data_dir, "Test_data.csv")
df_te = pd.read_csv(test_data)

adaptation_data = os.path.join(data_dir, "Adaptation_data.csv")
df_ad = pd.read_csv(adaptation_data)

# Extract site ids from test and adaptation data
site_ids_te =  sorted(set(df_te["Site"].to_list()))

site_ids_ad =  sorted(set(df_ad["Site"].to_list()))



# NB: In the test data the IDPs uses "_and_" while the model uses "&". 
# Therefore, it is necessary to change the column names in the test data. 
# The txt files containing idps were changed manually. column names are changed in python
df_te.columns = df_te.columns.str.replace("_and_", "&", regex=True)
df_ad.columns = df_ad.columns.str.replace("_and_", "&", regex=True)

# Check if all test sites are in the adaptation data
if not all(elem in site_ids_ad for elem in site_ids_te):
    print("Warning: some of the testing sites are not in the adaptation data")
    
# From the pretrained model, load the list of IDPs for left and right hemisphere plus subcortical regions
# NB: IDPs not present in training data were removed from subcortical text file
with open(os.path.join(data_dir, "P_phenotypes_ct_lh.txt")) as f:
    idp_ids_lh = f.read().splitlines()
with open(os.path.join(data_dir, "P_phenotypes_ct_rh.txt")) as f:
    idp_ids_rh = f.read().splitlines()
with open(os.path.join(data_dir, "P_phenotypes_sc.txt")) as f:
    idp_ids_sc = f.read().splitlines()

# Choose to process all IDPs
idp_ids = idp_ids_lh + idp_ids_rh + idp_ids_sc

# Choose covariate data columns
cols_cov = ["Age","Sex"]

# Set limits for cubic B-spline basis 
xmin = -5 
xmax = 110

# Select absolute Z treshold above which a sample is considered to be an outlier (without fitting any model)
outlier_thresh = 7

            ### RUN NORMATIVE MODEL ###

# Make predictions for HC subjects
df_te_HC = df_te.loc[df_te["Group"]==0]
for idp_num, idp in enumerate(idp_ids): 
    print("Running IDP", idp_num, idp, ":")
    idp_dir = os.path.join(out_dir, idp)
    os.chdir(idp_dir)
    
    # Extract and save response variables for test data
    y_te_HC = df_te_HC[idp].to_numpy()
    
    # Save the variables
    resp_file_te_HC = os.path.join(idp_dir, "resp_te_HC.txt") 
    np.savetxt(resp_file_te_HC, y_te_HC)
        
    # Configure and save the design matrix
    cov_file_te_HC = os.path.join(idp_dir, "cov_bspline_te_HC.txt")
    X_te_HC = create_design_matrix(df_te_HC[cols_cov], 
                                site_ids = df_te_HC["Site"],
                                all_sites = site_ids_tr,
                                basis = "bspline", 
                                xmin = xmin, 
                                xmax = xmax)
    np.savetxt(cov_file_te_HC, X_te_HC)
    
    # Check whether all test sites are represented in the training data
    if all(elem in site_ids_tr for elem in site_ids_te):
        print("All sites are present in the training data")
        
        # Make predictions for test data
        yhat_te_HC, s2_te_HC, Z_HC = predict(cov_file_te_HC, 
                                    alg= "blr", 
                                    respfile=resp_file_te_HC, 
                                    model_path=os.path.join(idp_dir,"models"),
                                    outputsuffix="_HC")
    else:
        print("Some sites missing from the training data. Adapting model")
        
        # Save covariates for adaptation data
        X_ad = create_design_matrix(df_ad[cols_cov], 
                                    site_ids = df_ad["Site"],
                                    all_sites = site_ids_tr,
                                    basis = "bspline", 
                                    xmin = xmin, 
                                    xmax = xmax)
        cov_file_ad = os.path.join(idp_dir, "cov_bspline_ad.txt")          
        np.savetxt(cov_file_ad, X_ad)
        
        # Save responses for adaptation data
        resp_file_ad = os.path.join(idp_dir, "resp_ad.txt") 
        y_ad = df_ad[idp].to_numpy()
        np.savetxt(resp_file_ad, y_ad)
       
        # Save site ids for adaptation data
        sitenum_file_ad = os.path.join(idp_dir, "sitenum_ad.txt") 
        site_num_ad = df_ad["Sitenum"].to_numpy(dtype=int)
        np.savetxt(sitenum_file_ad, site_num_ad)
        
        # Save site ids for test data 
        sitenum_file_te_HC = os.path.join(idp_dir, "sitenum_te_HC.txt")
        site_num_te_HC = df_te_HC["Sitenum"].to_numpy(dtype=int)
        np.savetxt(sitenum_file_te_HC, site_num_te_HC)
        #Make predictions
        yhat_te_HC, s2_te_HC, Z_HC = predict(cov_file_te_HC, 
                                    alg = "blr", 
                                    respfile = resp_file_te_HC, 
                                    model_path = os.path.join(idp_dir,"models"),
                                    adaptrespfile = resp_file_ad,
                                    adaptcovfile = cov_file_ad,
                                    adaptvargroupfile = sitenum_file_ad,
                                    testvargroupfile = sitenum_file_te_HC,
                                    outputsuffix="_HC")
        
# Make predictions for patients (FEP subjects)
df_te_Pt = df_te.loc[df_te["Group"]==1]
for idp_num, idp in enumerate(idp_ids): 
    print("Running IDP", idp_num, idp, ":")
    idp_dir = os.path.join(out_dir, idp)
    os.chdir(idp_dir)
    
    # Extract and save respoonse variables for test data
    y_te_Pt = df_te_Pt[idp].to_numpy()
    
    # Save the variables
    resp_file_te_Pt = os.path.join(idp_dir, "resp_te_Pt.txt") 
    np.savetxt(resp_file_te_Pt, y_te_Pt)
        
    # Configure and save the design matrix
    cov_file_te_Pt = os.path.join(idp_dir, "cov_bspline_te_Pt.txt")
    X_te_Pt = create_design_matrix(df_te_Pt[cols_cov], 
                                site_ids = df_te_Pt["Site"],
                                all_sites = site_ids_tr,
                                basis = "bspline", 
                                xmin = xmin, 
                                xmax = xmax)
    np.savetxt(cov_file_te_Pt, X_te_Pt)
    
    #  Check whether all test sites are represented in the training data
    if all(elem in site_ids_tr for elem in site_ids_te):
        print("All sites are present in the training data")
        
        # Make predictions for test data
        yhat_te_Pt, s2_te_Pt, Z_Pt = predict(cov_file_te_Pt, 
                                    alg= "blr", 
                                    respfile=resp_file_te_Pt, 
                                    model_path=os.path.join(idp_dir,"models"),
                                    outputsuffix="_Pt")
    else:
        print("Some sites missing from the training data. Adapting model")
        
        # Save covariates for adaptation data
        X_ad = create_design_matrix(df_ad[cols_cov], 
                                    site_ids = df_ad["Site"],
                                    all_sites = site_ids_tr,
                                    basis = "bspline", 
                                    xmin = xmin, 
                                    xmax = xmax)
        cov_file_ad = os.path.join(idp_dir, "cov_bspline_ad.txt")          
        np.savetxt(cov_file_ad, X_ad)
        
        # Save responses for adaptation data
        resp_file_ad = os.path.join(idp_dir, "resp_ad.txt") 
        y_ad = df_ad[idp].to_numpy()
        np.savetxt(resp_file_ad, y_ad)
       
        # Save site ids for adaptation data
        sitenum_file_ad = os.path.join(idp_dir, "sitenum_ad.txt") 
        site_num_ad = df_ad["Sitenum"].to_numpy(dtype=int)
        np.savetxt(sitenum_file_ad, site_num_ad)
        
        # Save site ids for test data 
        sitenum_file_te_Pt = os.path.join(idp_dir, "sitenum_te_Pt.txt")
        site_num_te_Pt = df_te_Pt["Sitenum"].to_numpy(dtype=int)
        np.savetxt(sitenum_file_te_Pt, site_num_te_Pt)
        # Make predictions
        yhat_te_Pt, s2_te_Pt, Z_Pt = predict(cov_file_te_Pt, 
                                    alg = "blr", 
                                    respfile = resp_file_te_Pt, 
                                    model_path = os.path.join(idp_dir,"models"),
                                    adaptrespfile = resp_file_ad,
                                    adaptcovfile = cov_file_ad,
                                    adaptvargroupfile = sitenum_file_ad,
                                    testvargroupfile = sitenum_file_te_Pt,
                                    outputsuffix="_Pt")
        
    

            ### Create plots ###
        
# Prepare dummy data for plotting
# Choose which sex to plot
sex = 0  # 1 = male 0 = female
if sex == 1: 
    clr = "blue"
    clr2 = "mediumblue";
else:
    clr = "red"
    clr2 = "darkred"

# Create dummy data for visualisation
print("configuring dummy data ...")
xx = np.arange(xmin, xmax, 0.5)
X0_dummy = np.zeros((len(xx), 2))
X0_dummy[:,0] = xx
X0_dummy[:,1] = sex

# Create the design matrix
X_dummy = create_design_matrix(X0_dummy, xmin=xmin, xmax=xmax, site_ids=None, all_sites=site_ids_tr)

# Save the dummy covariates
cov_file_dummy = os.path.join(out_dir,"cov_bspline_dummy_mean.txt")
np.savetxt(cov_file_dummy, X_dummy)

# Define this to avoid duplicate labels in legend
def legend_without_duplicate_labels(figure):
    handles, labels = plt.gca().get_legend_handles_labels()
    by_label = dict(zip(labels, handles))
    figure.legend(by_label.values(), by_label.keys(), loc='upper right')

# Plot the normative model
sns.set(style="whitegrid")

for idp_num, idp in enumerate(idp_ids): 
    print("Running IDP", idp_num, idp, ':')
    idp_dir = os.path.join(out_dir, idp)
    os.chdir(idp_dir)
    
    # Load the true data points
    yhat_te_HC = load_2d(os.path.join(idp_dir, "yhat_HC.txt"))
    s2_te_HC = load_2d(os.path.join(idp_dir, "ys2_HC.txt"))
    y_te_HC = load_2d(os.path.join(idp_dir, "resp_te_HC.txt"))
    
    yhat_te_Pt = load_2d(os.path.join(idp_dir, "yhat_Pt.txt"))
    s2_te_Pt = load_2d(os.path.join(idp_dir, "ys2_Pt.txt"))
    y_te_Pt = load_2d(os.path.join(idp_dir, "resp_te_Pt.txt"))
            
    # Set up covariates for dummy data
    print("Making predictions with dummy covariates (for visualisation)")
    yhat, s2 = predict(cov_file_dummy, 
                       alg = "blr", 
                       respfile = None, 
                       model_path = os.path.join(idp_dir,"Models"), 
                       outputsuffix = "_dummy")
    
    # Load the normative model
    with open(os.path.join(idp_dir,"Models", "NM_0_0_estimate.pkl"), "rb") as handle:
        nm = pickle.load(handle) 
    
    # Get the warp and warp parameters
    W = nm.blr.warp
    warp_param = nm.blr.hyp[1:nm.blr.warp.get_n_params()+1] 
        
    # Warp predictions for the true data and compute evaluation metrics
    med_te_HC = W.warp_predictions(np.squeeze(yhat_te_HC), np.squeeze(s2_te_HC), warp_param)[0]
    med_te_HC = med_te_HC[:, np.newaxis]
    print("metrics:", evaluate(y_te_HC, med_te_HC))
    
    med_te_Pt = W.warp_predictions(np.squeeze(yhat_te_Pt), np.squeeze(s2_te_Pt), warp_param)[0]
    med_te_Pt = med_te_Pt[:, np.newaxis]
    print("metrics:", evaluate(y_te_Pt, med_te_Pt))  
    
    # Warp dummy predictions to create the plots
    med, pr_int = W.warp_predictions(np.squeeze(yhat), np.squeeze(s2), warp_param)
    
    # Extract the different variance components to visualise
    beta, junk1, junk2 = nm.blr._parse_hyps(nm.blr.hyp, X_dummy)
    s2n = 1/beta # variation (aleatoric uncertainty)
    s2s = s2-s2n # modelling uncertainty (epistemic uncertainty)
    
    # Plot the data points
    y_te_HC_rescaled_all = np.zeros_like(y_te_HC)
    y_te_Pt_rescaled_all = np.zeros_like(y_te_Pt)
    for sid, site in enumerate(site_ids_te):
        # Plot the true test data points 
        if all(elem in site_ids_tr for elem in site_ids_te):
            # All data in the test set are present in the training set
            
            # Select the data points belonging to the particular site
            idx_HC = np.where(np.bitwise_and(X_te_HC[:,2] == sex, X_te_HC[:,sid+len(cols_cov)+1] !=0))[0]
            if len(idx_HC) == 0:
                print("No HC data for site", sid, site, "skipping...")
                continue
            idx_Pt = np.where(np.bitwise_and(X_te_Pt[:,2] == sex, X_te_Pt[:,sid+len(cols_cov)+1] !=0))[0]
            if len(idx_Pt) == 0:
                print("No Pt data for site", sid, site, "skipping...")
                continue
            
            # Directly adjust the data
            idx_dummy_HC = np.bitwise_and(X_dummy[:,1] > X_te_HC[idx_HC,1].min(), X_dummy[:,1] < X_te_HC[idx_HC,1].max())
            y_te_HC_rescaled = y_te_HC[idx_HC] - np.median(y_te_HC[idx_HC]) + np.median(med[idx_dummy])
            
            idx_dummy_Pt = np.bitwise_and(X_dummy[:,1] > X_te_Pt[idx_Pt,1].min(), X_dummy[:,1] < X_te_Pt[idx_Pt,1].max())
            y_te_Pt_rescaled = y_te_Pt[idx_Pt] - np.median(y_te_Pt[idx_Pt]) + np.median(med[idx_dummy])
        else:
            # Adjust the data based on the adaptation dataset 
            
            # Select the data point belonging to this particular site
            idx_HC = np.where(np.bitwise_and(X_te_HC[:,2] == sex, (df_te_HC["Site"] == site).to_numpy()))[0]
            idx_Pt = np.where(np.bitwise_and(X_te_Pt[:,2] == sex, (df_te_Pt["Site"] == site).to_numpy()))[0]
            
            # Load the adaptation data
            y_ad = load_2d(os.path.join(idp_dir, "resp_ad.txt"))
            X_ad = load_2d(os.path.join(idp_dir, "cov_bspline_ad.txt"))
            idx_a = np.where(np.bitwise_and(X_ad[:,2] == sex, (df_ad["Site"] == site).to_numpy()))[0]
            if len(idx_HC) < 2 or len(idx_a) < 2:
                print("Insufficent data for site", sid, site, "skipping...")
                continue
            if len(idx_Pt) < 2 or len(idx_a) < 2:
                print("Insufficent data for site", sid, site, "skipping...")
                continue         
            
            # Adjust and rescale the test data
            y_te_HC_rescaled, s2_rescaled = nm.blr.predict_and_adjust(nm.blr.hyp, 
                                                                   X_ad[idx_a,:], 
                                                                   np.squeeze(y_ad[idx_a]), 
                                                                   Xs=None, 
                                                                   ys=np.squeeze(y_te_HC[idx_HC]))
            
            y_te_Pt_rescaled, s2_rescaled = nm.blr.predict_and_adjust(nm.blr.hyp, 
                                                                   X_ad[idx_a,:], 
                                                                   np.squeeze(y_ad[idx_a]), 
                                                                   Xs=None, 
                                                                   ys=np.squeeze(y_te_Pt[idx_Pt]))
        # Plot the (adjusted) data points
        plt.scatter(X_te_HC[idx_HC,1], y_te_HC_rescaled, s=22, color=clr2, marker='o', alpha = 1, label="HC")
        plt.scatter(X_te_Pt[idx_Pt,1], y_te_Pt_rescaled, s=24, color='black', marker= 'x', alpha=0.7, label="FEP")
       
        legend_without_duplicate_labels(plt)
        
    # Plot the median of the dummy data
    plt.plot(xx, med, clr)
    
    # Fill the gaps in between the centiles
    junk, pr_int25 = W.warp_predictions(np.squeeze(yhat), np.squeeze(s2), warp_param, percentiles=[0.25,0.75])
    junk, pr_int95 = W.warp_predictions(np.squeeze(yhat), np.squeeze(s2), warp_param, percentiles=[0.05,0.95])
    junk, pr_int99 = W.warp_predictions(np.squeeze(yhat), np.squeeze(s2), warp_param, percentiles=[0.01,0.99])
    plt.fill_between(xx, pr_int25[:,0], pr_int25[:,1], alpha = 0.1,color=clr)
    plt.fill_between(xx, pr_int95[:,0], pr_int95[:,1], alpha = 0.1,color=clr)
    plt.fill_between(xx, pr_int99[:,0], pr_int99[:,1], alpha = 0.1,color=clr)
            
    # Make the width of each centile proportional to the epistemic uncertainty
    junk, pr_int25l = W.warp_predictions(np.squeeze(yhat), np.squeeze(s2-0.5*s2s), warp_param, percentiles=[0.25,0.75])
    junk, pr_int95l = W.warp_predictions(np.squeeze(yhat), np.squeeze(s2-0.5*s2s), warp_param, percentiles=[0.05,0.95])
    junk, pr_int99l = W.warp_predictions(np.squeeze(yhat), np.squeeze(s2-0.5*s2s), warp_param, percentiles=[0.01,0.99])
    junk, pr_int25u = W.warp_predictions(np.squeeze(yhat), np.squeeze(s2+0.5*s2s), warp_param, percentiles=[0.25,0.75])
    junk, pr_int95u = W.warp_predictions(np.squeeze(yhat), np.squeeze(s2+0.5*s2s), warp_param, percentiles=[0.05,0.95])
    junk, pr_int99u = W.warp_predictions(np.squeeze(yhat), np.squeeze(s2+0.5*s2s), warp_param, percentiles=[0.01,0.99])    
    plt.fill_between(xx, pr_int25l[:,0], pr_int25u[:,0], alpha = 0.3,color=clr)
    plt.fill_between(xx, pr_int95l[:,0], pr_int95u[:,0], alpha = 0.3,color=clr)
    plt.fill_between(xx, pr_int99l[:,0], pr_int99u[:,0], alpha = 0.3,color=clr)
    plt.fill_between(xx, pr_int25l[:,1], pr_int25u[:,1], alpha = 0.3,color=clr)
    plt.fill_between(xx, pr_int95l[:,1], pr_int95u[:,1], alpha = 0.3,color=clr)
    plt.fill_between(xx, pr_int99l[:,1], pr_int99u[:,1], alpha = 0.3,color=clr)

    # Plot actual centile lines
    plt.plot(xx, pr_int25[:,0],color=clr, linewidth=0.5)
    plt.plot(xx, pr_int25[:,1],color=clr, linewidth=0.5)
    plt.plot(xx, pr_int95[:,0],color=clr, linewidth=0.5)
    plt.plot(xx, pr_int95[:,1],color=clr, linewidth=0.5)
    plt.plot(xx, pr_int99[:,0],color=clr, linewidth=0.5)
    plt.plot(xx, pr_int99[:,1],color=clr, linewidth=0.5)
    
    plt.xlabel("Age")
    plt.ylabel(idp) 
    plt.title(idp)
    plt.xlim((15,50))
    plt.savefig(os.path.join(idp_dir, "centiles_" + str(sex)),  bbox_inches="tight")
    plt.show()
    

os.chdir(out_dir)


            ### Concatenate results ###
            
#Create folders containing all deviation score files and concatenate deviation scores into one CSV file
    # FOR HC subjects
os.chdir(out_dir)
! mkdir deviation_scores_HC

! for i in *; do if [[ -e ${i}/Z_HC.txt ]]; then cp ${i}/Z_HC.txt deviation_scores_HC/${i}_Z_HC.txt; fi; done

import shutil
filenames = os.listdir('/braincharts/models/lifespan_57K_82sites')
for i in filenames:
    if os.path.exists(i + '/Z_HC.txt'):
        shutil.copy2(i + '/Z_HC.txt', 'deviation_scores_HC/' + i + '_Z_HC.txt')

z_dir = "/braincharts/models/" + model_name + "/deviation_scores_HC/"
filelist = [name for name in os.listdir(z_dir)]

os.chdir(z_dir)
Z_HC_df = pd.concat([pd.read_csv(item, names=[item[:-4]]) for item in filelist], axis=1)


df_te_HC.reset_index(inplace=True)

Z_HC_df["Id"] = df_te_HC["Id"]

df_te_HC_Z = pd.merge(df_te_HC, Z_HC_df, on="Id", how="inner")

df_te_HC_Z = df_te_HC_Z.drop('index', axis=1)

df_te_HC_Z.to_csv("Baseline_HC_deviation_scores.csv", index=False)

    # For patients (FEP subjects)
os.chdir(out_dir)

! mkdir deviation_scores_Pt

! for i in *; do if [[ -e ${i}/Z_Pt.txt ]]; then cp ${i}/Z_Pt.txt deviation_scores_Pt/${i}_Z_Pt.txt; fi; done

import shutil
filenames = os.listdir('/braincharts/models/lifespan_57K_82sites')
for i in filenames:
    if os.path.exists(i + '/Z_Pt.txt'):
        shutil.copy2(i + '/Z_Pt.txt', 'deviation_scores_Pt/' + i + '_Z_Pt.txt')

z_dir = "/braincharts/models/" + model_name + "/deviation_scores_Pt/"
filelist = [name for name in os.listdir(z_dir)]


os.chdir(z_dir)
Z_Pt_df = pd.concat([pd.read_csv(item, names=[item[:-4]]) for item in filelist], axis=1)

df_te_Pt.reset_index(inplace=True)

Z_Pt_df["Id"] = df_te_Pt["Id"]

df_te_Pt_Z = pd.merge(df_te_Pt, Z_Pt_df, on="Id", how="inner")

df_te_Pt_Z = df_te_Pt_Z.drop('index', axis=1)

df_te_Pt_Z.to_csv("Baseline_Pt_deviation_scores.csv", index=False)
